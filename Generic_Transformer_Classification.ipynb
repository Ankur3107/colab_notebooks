{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generic Transformer Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNwZCmHcOTCODTVymA00wO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankur3107/colab_notebooks/blob/master/Generic_Transformer_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABHu2x6o0SWq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "dd83f1c1-7e75-4408-f2ba-96b36911db3b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ76cDKo0bD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "from transformers import *\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPopMG6C1kgq",
        "colab_type": "text"
      },
      "source": [
        "# 1. Set Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-neKLU0zICc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config:\n",
        "  train_file = './data.csv'\n",
        "  eval_file = './eval.csv'\n",
        "  max_seq_len = 128\n",
        "  batch_size = 32\n",
        "  epochs = 5\n",
        "  model_name = 'bert-base-uncased'\n",
        "  learning_rate = 2e-5\n",
        "  n_classes = 3\n",
        "  device = 'cpu'\n",
        "  \n",
        "\n",
        "\n",
        "flags = Config"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4pywFOC1s3m",
        "colab_type": "text"
      },
      "source": [
        "# 2. Build Dataset Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVYil6kYzmTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextLabelDataset(Dataset):\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "  \n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          return_token_type_ids=False,\n",
        "          pad_to_max_length=True,\n",
        "          return_attention_mask=True,\n",
        "          return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "          'texts': text,\n",
        "          'input_ids': encoding['input_ids'].flatten(),\n",
        "          'attention_mask': encoding['attention_mask'].flatten(),\n",
        "          'targets': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size, is_prediction=False):\n",
        "\n",
        "  if isinstance(df, str):\n",
        "    df = pd.read_csv(df)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  if is_prediction:\n",
        "    ds = TextLabelDataset(\n",
        "        texts=df.text.to_numpy(),\n",
        "        labels=np.array([-1]*len(df.text.values)),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "        )\n",
        "  else:\n",
        "    ds = TextLabelDataset(\n",
        "        texts=df.text.to_numpy(),\n",
        "        labels=df.labels.to_numpy(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "        )\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=4\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoFD_xtB1qzO",
        "colab_type": "text"
      },
      "source": [
        "# 3. Build Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLe0S5U4zot_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "\n",
        "  def __init__(self, model_name, n_classes):\n",
        "      super(Classifier, self).__init__()\n",
        "      self.bert = AutoModel.from_pretrained(model_name)\n",
        "      self.drop = nn.Dropout(p=0.3)\n",
        "      self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "      _, pooled_output = self.bert(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      output = self.drop(pooled_output)\n",
        "      return self.out(output)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHsf_x2SzqKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassificationModel:\n",
        "\n",
        "  def __init__(self, flags):\n",
        "    self.flags = flags\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(self.flags.model_name)\n",
        "    self.model = Classifier(self.flags.model_name, self.flags.n_classes)\n",
        "    self.model = self.model.to(self.flags.device)\n",
        "\n",
        "  def train():\n",
        "\n",
        "    train_data_loader = create_data_loader(self.flags.train_file, self.okenizer, self.flags.max_seq_len, self.flags.batch_size)\n",
        "    val_data_loader = create_data_loader(self.flags.eval_file, self.tokenizer, self.flags.max_seq_len, self.flags.batch_size)\n",
        "\n",
        "    optimizer = AdamW(self.model.parameters(), lr=self.flags.learning_rate, correct_bias=False)\n",
        "    total_steps = len(train_data_loader) * self.flags.epochs\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "      optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss().to(self.flags.device)\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for epoch in self.flags.epochs:\n",
        "\n",
        "      print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "      print('-' * 10)\n",
        "\n",
        "      train_acc, train_loss = self.train_epoch(\n",
        "        self.model,\n",
        "        train_data_loader,    \n",
        "        loss_fn, \n",
        "        optimizer, \n",
        "        self.flags.device, \n",
        "        scheduler, \n",
        "        len(train_df)\n",
        "      )\n",
        "\n",
        "      print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "      val_acc, val_loss = eval_model(\n",
        "        classifier_model,\n",
        "        val_data_loader,\n",
        "        loss_fn, \n",
        "        device, \n",
        "        len(eval_df)\n",
        "      )\n",
        "\n",
        "      print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "      print()\n",
        "\n",
        "      history['train_acc'].append(train_acc)\n",
        "      history['train_loss'].append(train_loss)\n",
        "      history['val_acc'].append(val_acc)\n",
        "      history['val_loss'].append(val_loss)\n",
        "\n",
        "      if val_acc > best_accuracy:\n",
        "        torch.save(classifier_model.state_dict(), 'best_model_state.bin')\n",
        "        best_accuracy = val_acc\n",
        "\n",
        "\n",
        "  def train_epoch(self, model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "    model = model.train()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "  def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "    model = model.eval()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"targets\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask\n",
        "        )\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi018rQ9M1p0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}